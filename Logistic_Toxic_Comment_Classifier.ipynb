{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report as class_report\n",
    "from sklearn.linear_model import SGDClassifier as SGD\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Takes a data frame and splits it into a training and testing set based on given proportion.\n",
    "\n",
    "Parameters:\n",
    "    df: The data frame to split.\n",
    "    prop: The proportion of data to be used in the training set.\n",
    "    \n",
    "Return:\n",
    "    train: Training data set\n",
    "    test: Testing data set\n",
    "\"\"\"\n",
    "def basicSplit(df,prop, random_state):\n",
    "    train = df.sample(frac = prop, random_state = random_state)\n",
    "    test = df.loc[~df.index.isin(train.index)]\n",
    "    return train,test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Splits the data set into training and testing. Toxic comments are upsampled to give more weight\n",
    "to toxic comment classification.\n",
    "\n",
    "Parameters:\n",
    "    df: The data frame to split.\n",
    "    prop: The proportion of data to be used in the training set.\n",
    "    random_state: Fixes randomization of sampling\n",
    "    \n",
    "Return:\n",
    "    train: Training data set with equal proportions toxic and non toxic data\n",
    "    test: Testing data set\n",
    "\"\"\"\n",
    "\n",
    "def splitDF_up_sample_toxic (df,prop, random_state = 0):\n",
    "    \n",
    "    train, test = basicSplit(df,prop, random_state = 0) #Split the data frame up\n",
    "    \n",
    "    trainToxic = train.loc[train[\"toxic\"] == 1]#get all toxic comments from training set\n",
    "    #print(trainToxic)\n",
    "    trainNotToxic = train.loc[~train.index.isin(trainToxic.index)]#get all non toxic comments from training\n",
    "    train = trainNotToxic.append(resample(trainToxic,n_samples= len(trainNotToxic.index),random_state = random_state))\n",
    "    return train,test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gets the word vector counts of the data sets for the logistic model\n",
    "Parameters\n",
    "    df: The data frame to process (usually traing or testing data frame)\n",
    "    vocab: Previous vocab to use (use training vocab for testing, use preselected words if words have been selected)\n",
    "    lowercase: Whether or not to take the lowercase of the words\n",
    "    \n",
    "Returns: \n",
    "    TFVect: Vector of word counts for each comment\n",
    "    vocab: List of words used\n",
    "\"\"\"\n",
    "def process(df, vocab = None, lowercase = True):\n",
    "    \n",
    "    cv = CountVectorizer(stop_words = {\"english\"},lowercase=lowercase,vocabulary = vocab)#get count vectorizer\n",
    "    wordVector = cv.fit_transform(df[\"comment_text\"]) #get count vector of words\n",
    "    tfidf_transformer = TfidfTransformer() #make tfidf_transformer\n",
    "    TFVect = tfidf_transformer.fit_transform(wordVector)#perform tfidf transform\n",
    "    \n",
    "    if vocab == None: #get the list of vocab words if preset vocab was not used\n",
    "        vocab = cv.get_feature_names()\n",
    "        \n",
    "    return TFVect,vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prints out the count of misclassified toxic and not toxic comments.\n",
    "\n",
    "Parameters:\n",
    "    test: The test data set\n",
    "    predict: The predictions made by the model\n",
    "    tag: The type of comment classified (toxic, obscene, ect.)\n",
    "\"\"\"\n",
    "\n",
    "def CountPredict (test,predict,tag):\n",
    "    \n",
    "    test.index = [x for x in range(0,len(test.index))] #reset the test index to make iteration easier\n",
    "    \n",
    "    count1 = 0 #total non-toxic count\n",
    "    count2 = 0 #total toxic count\n",
    "    count1f = 0 #Count of non-toxic comments classified as toxic\n",
    "    count2f = 0 #Count of toxic comments classified as non-toxic\n",
    "    \n",
    "    #count toxic and non toxic data\n",
    "    for i in range(len(predict)):\n",
    "        \n",
    "        if test.loc[i,tag] == 0:\n",
    "                count1 +=1 \n",
    "                if predict[i] != test.loc[i,tag]:\n",
    "                    count1f += 1\n",
    "                    \n",
    "        if test.loc[i,tag] ==1:\n",
    "            count2 += 1\n",
    "            if predict[i] != test.loc[i,tag]:\n",
    "                count2f +=1\n",
    "                \n",
    "    print (\"Proportion Non-Toxic Misclassified: %i/%i\" %(count1f, count1), \"= \", (count1f/count1))\n",
    "    print (\"Proportion Toxic Misclassified: %i/%i\" %(count2f, count2), \"= \", (count2f/count2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find a good logistic model using a reduced number of words\n",
    "\n",
    "Parameters:\n",
    "    df: The data frame to build the model with.\n",
    "    tag: The category to model (toxic, obscene, ect).\n",
    "    \n",
    "Return:\n",
    "    bestFit: The best model found\n",
    "    bestWords: The words used in the model\n",
    "    test: The final test data set with predicted values appended\n",
    "\"\"\"\n",
    "\n",
    "def topWordslogReg_up_sample (df,tag):\n",
    "    \n",
    "    procAll, theVocab = process(df) #get the total vocab of data frame(used so that errors don't occur later)\n",
    "    \n",
    "    logistic = linear_model.LogisticRegression(penalty=\"l1\") #make logistic modeler\n",
    "   \n",
    "    bestScore = 0 #The best score scene in the models generated\n",
    "    bestFit = None #The model with the best score\n",
    "    bestVocab = [] #The vocab of the best model\n",
    "    \n",
    "    #make 5 models and pick the one with the highest score\n",
    "    for i in range(5):\n",
    "        train,test= splitDF_up_sample_toxic(df,.7, random_state = i)\n",
    "        procTrain,vocab1 = process(train,theVocab)\n",
    "        procTest,vocab = process(test,vocab1)\n",
    "        logfit = logistic.fit(procTrain, train[tag])\n",
    "        score = logfit.score(procTest,test[tag])\n",
    "        \n",
    "\n",
    "        if score > bestScore:\n",
    "            bestScore = score\n",
    "            bestFit = logfit\n",
    "            bestVocab = [x for x in vocab1]\n",
    "    \n",
    "    #coefficients should map to words in the vocab, Check if they are equal other wise \n",
    "    #words selectd may not be random\n",
    "    if len(bestVocab) != len(bestFit.coef_[0]):\n",
    "        print(\"Warning: Vocab and Coefficient length not equal, something went wrong\")\n",
    "        print(\"Length Best Vocab: \", len(bestVocab))\n",
    "        print(\"Length Logistic Coeficients: \", len(bestFit.coef_[0]))\n",
    "\n",
    "    print(\"Score of word selecting model: \",bestScore)\n",
    "    \n",
    "    #Select words that contribuited signifcantly to the prediction (abs(coefficent) > .5)\n",
    "    bestWords = []\n",
    "    wordTouple = []\n",
    "    for i in range(len(bestFit.coef_[0])):\n",
    "        if bestFit.coef_[0][i] >=.5 or bestFit.coef_[0][i] <= -.5:\n",
    "            bestWords.append(bestVocab[i])\n",
    "            \n",
    "    \n",
    "    print(\"Number of words: \", len(bestWords))\n",
    "    theVocab = bestWords\n",
    "    \n",
    "    #Pick the best out of 10 models using only the selected words\n",
    "    bestScore = 0\n",
    "    for i in range(10):\n",
    "        \n",
    "        train,test = splitDF_up_sample_toxic(df,.7, random_state = i+5)\n",
    "        procTrain,vocab1 = process(train,theVocab)\n",
    "        procTest,vocab = process(test,vocab1)\n",
    "        logfit = logistic.fit(procTrain, train[tag])\n",
    "        score = logfit.score(procTest,test[tag])\n",
    "        if score > bestScore or i == 0: #first\n",
    "            bestScore = score\n",
    "            bestFit = logfit\n",
    "            \n",
    "  \n",
    "    #run the prediction on the testing data of the last iteration of the loop for convience\n",
    "    print (\"Score of Final Model: \", bestScore)\n",
    "    predict = bestFit.predict(procTest)\n",
    "    CountPredict(test,predict,tag)\n",
    "    \n",
    "    #add the predected values to the test data frame and return it\n",
    "    newRow = \"predicted_\" + tag\n",
    "    test[newRow] = predict\n",
    "    \n",
    "    return bestFit, bestWords, test\n",
    "\n",
    "\n",
    "# In[41]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74251</th>\n",
       "      <td>c6a29bad26183dcf</td>\n",
       "      <td>\"\\nI haven't paraphrased you at all, Gary.  Yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131406</th>\n",
       "      <td>befd36e7acca9e56</td>\n",
       "      <td>I BLOCKED REVERS! I BLOCKED REVERS! I BLOCKED ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120969</th>\n",
       "      <td>8734c26db56d1763</td>\n",
       "      <td>I'm sorry. I'd like to unreservedly retract my...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121827</th>\n",
       "      <td>8bcf03120412d869</td>\n",
       "      <td>I don't know if this is exactly like the Press...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>0ca7b705720d6956</td>\n",
       "      <td>Thank you all, we'll all improve the Wikipedia...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "74251   c6a29bad26183dcf  \"\\nI haven't paraphrased you at all, Gary.  Yo...   \n",
       "131406  befd36e7acca9e56  I BLOCKED REVERS! I BLOCKED REVERS! I BLOCKED ...   \n",
       "120969  8734c26db56d1763  I'm sorry. I'd like to unreservedly retract my...   \n",
       "121827  8bcf03120412d869  I don't know if this is exactly like the Press...   \n",
       "4771    0ca7b705720d6956  Thank you all, we'll all improve the Wikipedia...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "74251       0             0        0       0       0              0  \n",
       "131406      1             0        0       0       0              0  \n",
       "120969      0             0        0       0       1              0  \n",
       "121827      0             0        0       0       0              0  \n",
       "4771        0             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_file = \"C:\\\\Users\\\\alexh\\\\Downloads\\\\toxic_comments_data.csv\" #May need to change path to find fild\n",
    "DF = pd.read_csv(DF_file)\n",
    "trainDF, testDF = basicSplit(DF,.7, random_state = 0)\n",
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0005300084f90edc</td>\n",
       "      <td>\"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0006f16e4e9f292e</td>\n",
       "      <td>Before you start throwing accusations and warn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>000bfd0867774845</td>\n",
       "      <td>\"\\nGood to know. About me, yeah, I'm studying ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>000ffab30195c5e1</td>\n",
       "      <td>Yes, because the mother of the child in the ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "2   000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "10  0005300084f90edc  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...   \n",
       "13  0006f16e4e9f292e  Before you start throwing accusations and warn...   \n",
       "21  000bfd0867774845  \"\\nGood to know. About me, yeah, I'm studying ...   \n",
       "27  000ffab30195c5e1  Yes, because the mother of the child in the ca...   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "2       0             0        0       0       0              0  \n",
       "10      0             0        0       0       0              0  \n",
       "13      0             0        0       0       0              0  \n",
       "21      0             0        0       0       0              0  \n",
       "27      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of word selecting model:  0.9389734407639511\n",
      "Number of words:  2727\n",
      "Score of Final Model:  0.9372127723067741\n",
      "Proportion Non-Toxic Misclassified: 1767/30334 =  0.05825146700072526\n",
      "Proportion Toxic Misclassified: 387/3176 =  0.12185138539042821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Create model from training data\n",
    "train_fit, bestWords, train_test_result = topWordslogReg_up_sample(trainDF,\"toxic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion Non-Toxic Misclassified: 2510/43290 =  0.05798105798105798\n",
      "Proportion Toxic Misclassified: 580/4581 =  0.12660991049989084\n"
     ]
    }
   ],
   "source": [
    "#Test model on test data\n",
    "procTest,vocab = process(testDF,bestWords)\n",
    "finalPredict = train_fit.predict(procTest)\n",
    "CountPredict(testDF,finalPredict,\"toxic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
